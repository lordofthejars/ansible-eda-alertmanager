---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/metrics
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: {{ .Release.Namespace }}
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: prometheus
    role: alert-rules
  name: prometheus-example-rules
spec:
  groups:
  - name: DemoAlerts
    # Note: all the rules below need to include the endpoint, namespace, and job
    # labels to ensure the EDA Pod receives the firing events. Maybe this can be
    # configured/filtered using some filtering logic on the alertmanager YAMLs?
    rules:
    - alert: InstanceDown 
      # If the replica count of the deployment drops below the given threshold,
      # scale the replica count up.
      expr: count by(job, endpoint, service, container, namespace) (up{job="quarkus-monitor"}) < 2
      for: 10s
    - alert: MemoryUsage
      # Sum the simulated "memory usage" from all replicas, and scale the replica
      # count of the Deployment if it exceeds the defined threshold, e.g scaling
      # replicas based on a backlog that can be consumed by multiple replicas
      expr: sum by(job, namespace, endpoint, service, container) (current_memory) / sum by(job, namespace, endpoint, service, container) (up{job="quarkus-monitor"}) > 20
      for: 10s
---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceAccountName: prometheus
  alerting:
    alertmanagers:
      - name: alertmanager-example
        namespace: {{ .Release.Namespace }}
        port: web
  serviceMonitorSelector:
    matchLabels:
      team: backend
  ruleSelector:
    matchLabels:
      role: alert-rules
      prometheus: prometheus
  resources:
    requests:
      memory: 400Mi
  enableAdminAPI: true
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
spec:
  type: NodePort
  ports:
  - name: web
    # nodePort: 30900
    port: 9090
    protocol: TCP
    targetPort: web
  selector:
    prometheus: prometheus
---
kind: Route
apiVersion: route.openshift.io/v1
metadata:
  name: prometheus
spec:
  to:
    kind: Service
    name: prometheus
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
  port:
    targetPort: web
